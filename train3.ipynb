{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh-keygen -t rsa -b 4096\n",
    "!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\n",
    "!cat /root/.ssh/id_rsa.pub\n",
    "# https://github.com/settings/ssh/new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"quangtd.24it@gmail.com\"\n",
    "!git config --global user.name \"quang\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone git@github.com:pewdspie24/Do_An.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from model.hsnet import HypercorrSqueezeNetwork\n",
    "from common.logger import Logger, AverageMeter\n",
    "from common.evaluation import Evaluator\n",
    "from common import utils\n",
    "from data.dataset import FSSDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mask_nshot(model, query_img, support_img, support_mask, shot):\n",
    "    logit_mask_agg = 0\n",
    "    predict_mask_agg = 0\n",
    "    for s_idx in range(shot):\n",
    "        # print(support_img.shape)\n",
    "        logit_mask = model(query_img, support_img[:, s_idx], support_mask[:, s_idx])\n",
    "        # print(logit_mask.shape)\n",
    "        # if self.use_original_imgsize:\n",
    "        #     org_qry_imsize = tuple([batch['org_query_imsize'][1].item(), batch['org_query_imsize'][0].item()])\n",
    "        #     logit_mask = F.interpolate(logit_mask, org_qry_imsize, mode='bilinear', align_corners=True)\n",
    "\n",
    "        logit_mask_agg += logit_mask.clone()\n",
    "        predict_mask_agg += logit_mask.argmax(dim=1).clone()\n",
    "        if shot == 1: return (logit_mask_agg, predict_mask_agg)\n",
    "\n",
    "    # Average & quantize predictions given threshold (=0.5)\n",
    "    bsz = predict_mask_agg.size(0)\n",
    "    max_vote = predict_mask_agg.view(bsz, -1).max(dim=1)[0]\n",
    "    # print(\"max_vote1\", predict_mask_agg.view(bsz, -1).shape)\n",
    "    max_vote = torch.stack([max_vote, torch.ones_like(max_vote).long()])\n",
    "    # print(\"max_vote2\", max_vote.shape)\n",
    "    max_vote = max_vote.max(dim=0)[0].view(bsz, 1, 1)\n",
    "    # print(\"max_vote3\", max_vote.shape)\n",
    "    pred_mask = predict_mask_agg.float() / max_vote\n",
    "    pred_mask[pred_mask < 0.5] = 0\n",
    "    pred_mask[pred_mask >= 0.5] = 1\n",
    "    logit_mask = logit_mask_agg / shot\n",
    "\n",
    "    return logit_mask, pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, dataloader, optimizer, training, shot):\n",
    "    r\"\"\" Train HSNet \"\"\"\n",
    "\n",
    "    # Force randomness during training / freeze randomness during testing\n",
    "    utils.fix_randseed(None) if training else utils.fix_randseed(0)\n",
    "    model.module.train_mode() if training else model.module.eval()\n",
    "    average_meter = AverageMeter(dataloader.dataset)\n",
    "\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "\n",
    "        # 1. Hypercorrelation Squeeze Networks forward pass\n",
    "        batch = utils.to_cuda(batch)\n",
    "        # print(batch['support_masks'].shape)\n",
    "        # logit_mask = model(batch['query_img'], batch['support_imgs'].squeeze(1), batch['support_masks'].squeeze(1), shot)\n",
    "        # pred_mask = logit_mask.argmax(dim=1)\n",
    "        query_img = batch['query_img']\n",
    "        support_img = batch['support_imgs']\n",
    "        support_mask = batch['support_masks']\n",
    "\n",
    "        logit_mask, pred_mask = train_mask_nshot(model, query_img, support_img, support_mask, shot)\n",
    "\n",
    "        # pred_mask = train_mask_nshot(model, query_img, support_img, support_mask, shot)\n",
    "\n",
    "        # 2. Compute loss & update model parameters\n",
    "        loss = model.module.compute_objective(logit_mask, batch['query_mask'])\n",
    "        # loss = model.module.compute_objective(logit_mask, batch['query_mask'])\n",
    "        if training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 3. Evaluate prediction\n",
    "        area_inter, area_union = Evaluator.classify_prediction(pred_mask, batch)\n",
    "        average_meter.update(area_inter, area_union, batch['class_id'], loss.detach().clone())\n",
    "        average_meter.write_process(idx, len(dataloader), epoch, write_batch_idx=50)\n",
    "\n",
    "    # Write evaluation results\n",
    "    average_meter.write_result('Training' if training else 'Validation', epoch)\n",
    "    avg_loss = utils.mean(average_meter.loss_buf)\n",
    "    miou, fb_iou = average_meter.compute_iou()\n",
    "\n",
    "    return avg_loss, miou, fb_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Hypercorrelation Squeeze Pytorch Implementation')\n",
    "parser.add_argument('--datapath', type=str, default='/content/drive/MyDrive/DO_AN/Data')\n",
    "parser.add_argument('--benchmark', type=str, default='custom', choices=['pascal', 'coco', 'fss', 'custom'])\n",
    "parser.add_argument('--logpath', type=str, default='/content/drive/MyDrive/DO_AN/log/newD_oldW_trans_5shot_COLAB')\n",
    "parser.add_argument('--bsz', type=int, default=2)\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--niter', type=int, default=2000)\n",
    "parser.add_argument('--nworker', type=int, default=2)\n",
    "parser.add_argument('--nwshot', type=int, default=3)\n",
    "parser.add_argument('--fold', type=int, default=0, choices=[0, 1, 2, 3])\n",
    "parser.add_argument('--backbone', type=str, default='resnet101', choices=['vgg16', 'resnet50', 'resnet101', 'resnet101_custom'])\n",
    "parser.add_argument('--resume', type=bool, default=False)\n",
    "args = parser.parse_args()\n",
    "Logger.initialize(args, training=True)\n",
    "\n",
    "# Model initialization\n",
    "model = HypercorrSqueezeNetwork(args.backbone, False)\n",
    "Logger.log_params(model)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "Logger.info('# available GPUs: %d' % torch.cuda.device_count())\n",
    "model = nn.DataParallel(model)\n",
    "model.to(device)\n",
    "\n",
    "# Helper classes (for training) initialization\n",
    "# optimizer = optim.Adam([{\"params\": model.parameters(), \"lr\": args.lr}])\n",
    "new_optimizer = optim.SGD([{\"params\": model.parameters(), \"lr\": args.lr, \"momentum\": 0.9}])\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(new_optimizer, 'max')\n",
    "\n",
    "\n",
    "######################### Enter previous epochs #############################\n",
    "init_eps = 0\n",
    "\n",
    "if args.resume:\n",
    "    # checkpoint = torch.load('logs/'+args.logpath+'.log/best_model.pt')\n",
    "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # new_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])   \n",
    "    # lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    # init_eps = checkpoint['epoch']\n",
    "    model.load_state_dict(torch.load(os.path.join(args.logpath, \"best_model.pt\")))\n",
    "Evaluator.initialize()\n",
    "\n",
    "# Dataset initialization\n",
    "FSSDataset.initialize(img_size=400, datapath=args.datapath, use_original_imgsize=False)\n",
    "dataloader_trn = FSSDataset.build_dataloader(args.benchmark, args.bsz, args.nworker, args.fold, 'trn', args.nwshot)\n",
    "dataloader_val = FSSDataset.build_dataloader(args.benchmark, args.bsz, args.nworker, args.fold, 'val', args.nwshot)\n",
    "\n",
    "# Train HSNet\n",
    "best_val_miou = float('-inf')\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(init_eps, args.niter):\n",
    "\n",
    "    trn_loss, trn_miou, trn_fb_iou = train(epoch, model, dataloader_trn, new_optimizer, training=True, shot=args.nwshot)\n",
    "    with torch.no_grad():\n",
    "        val_loss, val_miou, val_fb_iou = train(epoch, model, dataloader_val, new_optimizer, training=False, shot=args.nwshot)\n",
    "    lr_scheduler.step(val_miou)\n",
    "    # Save the best model\n",
    "    if val_miou > best_val_miou:\n",
    "        best_val_miou = val_miou\n",
    "        Logger.save_model_miou(model, epoch, val_miou, new_optimizer, lr_scheduler)\n",
    "    if epoch%25==0:\n",
    "        Logger.save_model_event(model, epoch, val_miou)\n",
    "\n",
    "    Logger.tbd_writer.add_scalars('data/loss', {'trn_loss': trn_loss, 'val_loss': val_loss}, epoch)\n",
    "    Logger.tbd_writer.add_scalars('data/miou', {'trn_miou': trn_miou, 'val_miou': val_miou}, epoch)\n",
    "    Logger.tbd_writer.add_scalars('data/fb_iou', {'trn_fb_iou': trn_fb_iou, 'val_fb_iou': val_fb_iou}, epoch)\n",
    "    Logger.tbd_writer.add_scalar('data/lr', new_optimizer.param_groups[0]['lr'], epoch)\n",
    "    Logger.tbd_writer.flush()\n",
    "Logger.tbd_writer.close()\n",
    "Logger.info('==================== Finished Training ====================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('hsnet2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13ef9a4ec3ac60f7557afcb7ba10f3a202d209dabacfed98de1ecf4ec416df5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
